{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2728e2b6",
   "metadata": {},
   "source": [
    "# Contribute to design of a self-driving car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd0313",
   "metadata": {},
   "source": [
    "To introduce the project, we have to perform image segmentation from Gerlany landscapes and town image dataset. Each of RGB image has its annotated images called a mask that identify the next eight classes : void, flat, human, sky, construction, nature, vehicle and object. The main goal is to build a model of Convolution neural networks that will take as input an RGB image to produce a predicted mask. The more the predicted mask will match with the true annotated one, the better the model will perform.\n",
    "This script should be use with the technical note (in the folder with file name \"ETIENNE_Louis_note_technique_11_2022\") to better understand the different approaches in building the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbae17e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os, json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "from keras_preprocessing.image import load_img\n",
    "from PIL import ImageOps, Image\n",
    "import imgaug.augmenters as iaa\n",
    "import imgaug as ia\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import layers\n",
    "from keras import metrics\n",
    "from keras.api._v2.keras.metrics import SparseCategoricalAccuracy\n",
    "import segmentation_models as sm \n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "from datetime import datetime as dt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99a0ac",
   "metadata": {},
   "source": [
    "# Images & annotated images Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a87df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cities = ['berlin', 'bielefeld', 'bonn', 'leverkusen', 'mainz', 'munich']\n",
    "train_cities = ['bochum', 'bremen', 'cologne', 'darmstadt', 'dusseldorf', 'erfurt', 'hamburg', 'hanover', 'jena', 'krefeld', \n",
    "                'monchengladbach', 'strasbourg', 'stuttgart', 'tubingen', 'ulm', 'weimar', 'zurich']\n",
    "val_cities = ['frankfurt', 'lindau', 'munster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3fe786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 2801\n",
      "Number of train annotations: 2801\n",
      "Number of val images: 500\n",
      "Number of val annotations: 500\n"
     ]
    }
   ],
   "source": [
    "train_img_paths = []\n",
    "train_ann_paths = []\n",
    "\n",
    "for cities in train_cities:\n",
    "    \n",
    "    train_img_dir = r\"D:\\Mes Documents\\Downloads\\leftImg8bit\\train/\" + cities\n",
    "    train_ann_dir = r\"D:\\Mes Documents\\Downloads\\P8_Cityscapes_gtFine_trainvaltest\\gtFine\\train/\" + cities\n",
    "    \n",
    "    train_img_paths = train_img_paths + sorted(\n",
    "        [\n",
    "            os.path.join(train_img_dir, fname)\n",
    "            for fname in os.listdir(train_img_dir)\n",
    "            if fname.endswith(\"_leftImg8bit.png\")\n",
    "        ]\n",
    "    )\n",
    "    train_ann_paths = train_ann_paths + sorted(\n",
    "        [\n",
    "            os.path.join(train_ann_dir, fname)\n",
    "            for fname in os.listdir(train_ann_dir)\n",
    "            if fname.endswith(\"_gtFine_labelIds.png\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(\"Number of train images:\", len(train_img_paths))\n",
    "print(\"Number of train annotations:\", len(train_ann_paths))\n",
    "\n",
    "\n",
    "val_img_paths = []\n",
    "val_ann_paths = []\n",
    "\n",
    "for cities in val_cities:\n",
    "    val_img_dir = r\"D:\\Mes Documents\\Downloads\\leftImg8bit\\val/\" + cities\n",
    "    val_ann_dir = r\"D:\\Mes Documents\\Downloads\\P8_Cityscapes_gtFine_trainvaltest\\gtFine\\val/\" + cities\n",
    "    \n",
    "\n",
    "\n",
    "    val_img_paths = val_img_paths + sorted(\n",
    "        [\n",
    "            os.path.join(val_img_dir, fname)\n",
    "            for fname in os.listdir(val_img_dir)\n",
    "            if fname.endswith(\"_leftImg8bit.png\")\n",
    "        ]\n",
    "    )\n",
    "    val_ann_paths = val_ann_paths + sorted(\n",
    "        [\n",
    "            os.path.join(val_ann_dir, fname)\n",
    "            for fname in os.listdir(val_ann_dir)\n",
    "            if fname.endswith(\"_gtFine_labelIds.png\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"Number of val images:\", len(val_img_paths))\n",
    "\n",
    "print(\"Number of val annotations:\", len(val_ann_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e9de9",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "389bebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = train_img_paths\n",
    "target_dir = train_ann_paths\n",
    "img_size = (160,160)\n",
    "num_classes = 8\n",
    "batch_size = 16\n",
    "augm_multiplier = 2 #coefficient of the number of times the input images will be produced during data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ea90a",
   "metadata": {},
   "source": [
    "## Define the 8 classes among the 33 sub-classes from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0142fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {'void': [0, 1, 2, 3, 4, 5, 6],\n",
    " 'flat': [7, 8, 9, 10],\n",
    " 'construction': [11, 12, 13, 14, 15, 16],\n",
    " 'object': [17, 18, 19, 20],\n",
    " 'nature': [21, 22],\n",
    " 'sky': [23],\n",
    " 'human': [24, 25],\n",
    " 'vehicle': [26, 27, 28, 29, 30, 31, 32, 33, -1]}\n",
    "\n",
    "def convertCats(x):\n",
    "    if x in cats['void']:\n",
    "        return 0\n",
    "    elif x in cats['flat']:\n",
    "        return 1\n",
    "    elif x in cats['construction']:\n",
    "        return 2\n",
    "    elif x in cats['object']:\n",
    "        return 3\n",
    "    elif x in cats['nature']:\n",
    "        return 4\n",
    "    elif x in cats['sky']:\n",
    "        return 5\n",
    "    elif x in cats['human']:\n",
    "        return 6\n",
    "    elif x in cats['vehicle']:\n",
    "        return 7\n",
    "\n",
    "convertCats_v = np.vectorize(convertCats) #to vectorize images array otherwise we can't get class of list of numbers 'x' with convertCats(x)\n",
    "\n",
    "def preprocessImg(img):\n",
    "    \"\"\" This function is a preprocess function that will make mask usable by numpy operations\"\"\"\n",
    "    image_matrix = np.expand_dims(img, 2)\n",
    "    converted_image = convertCats_v(image_matrix)\n",
    "\n",
    "    return converted_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada17678",
   "metadata": {},
   "source": [
    "# Prepare Sequence of data\n",
    "## Data augmentation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "354b2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([                                     \n",
    "\n",
    "    iaa.Sometimes(\n",
    "        0.5, iaa.GaussianBlur(sigma=(0, 0.1))),\n",
    "\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.97, 1.03), \"y\": (0.97, 1.03)},                # Zoom images to a value of 80 to 120% of their original size\n",
    "        translate_percent={\"x\": (-0.03, 0.03), \"y\": (-0.03, 0.03)},  # Translate images by -20 to 20% on x_axis and y_axis independently\n",
    "        rotate=(-25, 25))],                                          # Rotate images by -25 to 25% degrees\n",
    "         random_order=True)                                          # \"random_order = True\" means order of geometric augmentation is set randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee05df6",
   "metadata": {},
   "source": [
    "## DataGenerator\n",
    "Thanks to Keras.utils.Sequence class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16b19c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPets(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size * augm_multiplier,) + self.img_size + (3,), dtype=\"float32\") #la \",\" aprÃ¨s le nombre dans les \"()\" signifie que c'est un \"One element tuple\" et non un int\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size * augm_multiplier,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\") #img's shape = (2048,1024) --> without color_mode='grayscale', img would have shape of (2048,1024,3)\n",
    "            y[j] = preprocessImg(img) # \"np.expand_dims(img, 2)\" allows dimension expansion from (2048,1024) to (2048,1024,1) to get class number for each pixel\n",
    "\n",
    "        for multiplier in range(1, augm_multiplier):\n",
    "            for i in range(0, batch_size):\n",
    "                ia.seed(i)\n",
    "                img_augmentation = seq(image=x[i])\n",
    "                x[batch_size * multiplier + i] = img_augmentation\n",
    "\n",
    "                ia.seed(i)\n",
    "                target_augmentation = seq(image=y[i])\n",
    "                y[batch_size * multiplier + i] = target_augmentation\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20d445",
   "metadata": {},
   "source": [
    "# Define U_net model\n",
    "With custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f707a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 80, 80, 32)   896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 80, 80, 32)  128         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 80, 80, 32)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 80, 80, 32)   0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 80, 80, 64)  2400        ['activation_1[0][0]']           \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 80, 80, 64)  256         ['separable_conv2d[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 80, 80, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 80, 80, 64)  4736        ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 80, 80, 64)  256         ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 40, 40, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 40, 40, 64)   2112        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 40, 40, 64)   0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 40, 40, 64)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 40, 40, 128)  8896       ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 40, 40, 128)  512        ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 40, 40, 128)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 40, 40, 128)  17664      ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 40, 40, 128)  512        ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 20, 20, 128)  0          ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 20, 20, 128)  8320        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 20, 20, 128)  0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 20, 20, 128)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 20, 20, 256)  34176      ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 20, 20, 256)  1024       ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 20, 20, 256)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 20, 20, 256)  68096      ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 20, 20, 256)  1024       ['separable_conv2d_5[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 256)  0          ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 10, 10, 256)  33024       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 10, 10, 256)  0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 10, 10, 256)  0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 10, 10, 256)  590080     ['activation_7[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 10, 10, 256)  1024       ['conv2d_transpose[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 10, 10, 256)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 10, 10, 256)  590080     ['activation_8[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 10, 10, 256)  1024       ['conv2d_transpose_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 20, 20, 256)  0          ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 20, 20, 256)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 20, 20, 256)  65792       ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 20, 20, 256)  0           ['up_sampling2d[0][0]',          \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 20, 20, 256)  0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 20, 20, 128)  295040     ['activation_9[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 20, 20, 128)  512        ['conv2d_transpose_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 20, 20, 128)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 20, 20, 128)  147584     ['activation_10[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 20, 20, 128)  512        ['conv2d_transpose_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 40, 40, 256)  0          ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 40, 40, 128)  0          ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 40, 40, 128)  32896       ['up_sampling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 40, 40, 128)  0           ['up_sampling2d_2[0][0]',        \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 40, 40, 128)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 40, 40, 64)  73792       ['activation_11[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 40, 40, 64)  256         ['conv2d_transpose_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 40, 40, 64)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 40, 40, 64)  36928       ['activation_12[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 40, 40, 64)  256         ['conv2d_transpose_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSampling2D)  (None, 80, 80, 128)  0          ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSampling2D)  (None, 80, 80, 64)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 80, 80, 64)   8256        ['up_sampling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 80, 80, 64)   0           ['up_sampling2d_4[0][0]',        \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 80, 80, 64)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 80, 80, 32)  18464       ['activation_13[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 80, 80, 32)  128         ['conv2d_transpose_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 80, 80, 32)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 80, 80, 32)  9248        ['activation_14[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 80, 80, 32)  128         ['conv2d_transpose_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_7 (UpSampling2D)  (None, 160, 160, 64  0          ['add_5[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_6 (UpSampling2D)  (None, 160, 160, 32  0          ['batch_normalization_14[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 160, 160, 32  2080        ['up_sampling2d_7[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 160, 160, 32  0           ['up_sampling2d_6[0][0]',        \n",
      "                                )                                 'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 160, 160, 8)  2312        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,060,424\n",
      "Trainable params: 2,056,648\n",
      "Non-trainable params: 3,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03041edb",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "Kind of train/validation set of data preparation and load train and validation sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66aa545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our img paths into a training and a validation set\n",
    "val_samples = 400\n",
    "random.Random(1337).shuffle(train_img_paths)\n",
    "random.Random(1337).shuffle(train_ann_paths)\n",
    "train_input_img_paths = train_img_paths[:-val_samples]\n",
    "train_target_img_paths = train_ann_paths[:-val_samples]\n",
    "val_input_img_paths = val_img_paths[-val_samples:]\n",
    "val_target_img_paths = val_ann_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = OxfordPets(\n",
    "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
    ")\n",
    "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248eea6",
   "metadata": {},
   "source": [
    "## Define UpdatedMeanIoU\n",
    "To use with sparse classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c68410af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):\n",
    "  def __init__(self,\n",
    "               y_true=None,\n",
    "               y_pred=None,\n",
    "               num_classes=None,\n",
    "               ignore_class=None,\n",
    "               sparse_y_true: bool = True,\n",
    "               sparse_y_pred: bool = True,\n",
    "               axis: int = -1,\n",
    "               name=None,\n",
    "               dtype=None):\n",
    "    super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "    return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb8ff8c",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94ef603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Loulou\\anaconda3\\lib\\site-packages\\imgaug\\augmenters\\base.py:27: SuspiciousMultiImageShapeWarning: You provided a numpy array of shape (160, 160, 3) as a multi-image augmentation input, which was interpreted as (N, H, W). The last dimension however has value 1 or 3, which indicates that you provided a single image with shape (H, W, C) instead. If that is the case, you should use e.g. augmenter(image=<your input>) or augment_image(<your input>) -- note the singular 'image' instead of 'imageS'. Otherwise your single input image will be interpreted as multiple images of shape (H, W) during augmentation.\n",
      "  ia.warn(\n",
      "c:\\Users\\Loulou\\anaconda3\\lib\\site-packages\\imgaug\\augmenters\\base.py:27: SuspiciousMultiImageShapeWarning: You provided a numpy array of shape (160, 160, 1) as a multi-image augmentation input, which was interpreted as (N, H, W). The last dimension however has value 1 or 3, which indicates that you provided a single image with shape (H, W, C) instead. If that is the case, you should use e.g. augmenter(image=<your input>) or augment_image(<your input>) -- note the singular 'image' instead of 'imageS'. Otherwise your single input image will be interpreted as multiple images of shape (H, W) during augmentation.\n",
      "  ia.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 616s 8s/step - loss: 1.7419 - updated_mean_io_u: 0.3264 - sparse_categorical_accuracy: 0.7389 - val_loss: 2.5530 - val_updated_mean_io_u: 0.0851 - val_sparse_categorical_accuracy: 0.2291\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 563s 8s/step - loss: 0.6301 - updated_mean_io_u: 0.4274 - sparse_categorical_accuracy: 0.8197 - val_loss: 1.4604 - val_updated_mean_io_u: 0.1189 - val_sparse_categorical_accuracy: 0.4913\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 565s 8s/step - loss: 0.5240 - updated_mean_io_u: 0.4746 - sparse_categorical_accuracy: 0.8468 - val_loss: 1.4597 - val_updated_mean_io_u: 0.1263 - val_sparse_categorical_accuracy: 0.5094\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 564s 8s/step - loss: 0.4750 - updated_mean_io_u: 0.4992 - sparse_categorical_accuracy: 0.8586 - val_loss: 1.3529 - val_updated_mean_io_u: 0.1137 - val_sparse_categorical_accuracy: 0.5301\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 564s 8s/step - loss: 0.4354 - updated_mean_io_u: 0.5199 - sparse_categorical_accuracy: 0.8675 - val_loss: 1.3299 - val_updated_mean_io_u: 0.1303 - val_sparse_categorical_accuracy: 0.5617\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 563s 8s/step - loss: 0.3924 - updated_mean_io_u: 0.5412 - sparse_categorical_accuracy: 0.8797 - val_loss: 0.9423 - val_updated_mean_io_u: 0.2458 - val_sparse_categorical_accuracy: 0.7149\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 589s 8s/step - loss: 0.3653 - updated_mean_io_u: 0.5523 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.5866 - val_updated_mean_io_u: 0.3792 - val_sparse_categorical_accuracy: 0.8114\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 587s 8s/step - loss: 0.3392 - updated_mean_io_u: 0.5682 - sparse_categorical_accuracy: 0.8932 - val_loss: 0.4510 - val_updated_mean_io_u: 0.4853 - val_sparse_categorical_accuracy: 0.8569\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 587s 8s/step - loss: 0.3227 - updated_mean_io_u: 0.5760 - sparse_categorical_accuracy: 0.8971 - val_loss: 0.3769 - val_updated_mean_io_u: 0.5441 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 587s 8s/step - loss: 0.3082 - updated_mean_io_u: 0.5844 - sparse_categorical_accuracy: 0.9005 - val_loss: 0.3919 - val_updated_mean_io_u: 0.5457 - val_sparse_categorical_accuracy: 0.8774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x196ef98cf10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the model for training.\n",
    "# We use the \"sparse\" version of categorical_crossentropy\n",
    "# because our target data is integers and not one-hot vectors class\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[UpdatedMeanIoU(num_classes=8), keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 10\n",
    "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333e90a",
   "metadata": {},
   "source": [
    "# Segmenation with \"Segmentation.models\" (Unet / Linknet )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a5956",
   "metadata": {},
   "source": [
    "# Function pipeline segmentation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de68c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(\n",
    "    img_size,\n",
    "    train_img_paths=train_img_paths,\n",
    "    train_ann_paths=train_ann_paths,\n",
    "    val_img_paths=val_img_paths,\n",
    "    val_ann_paths=val_ann_paths):\n",
    "\n",
    "    train_img, train_ann, val_img, val_ann = [], [], [], []\n",
    "\n",
    "    for directory_path in train_img_paths[:]:\n",
    "        img = cv2.imread(directory_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        train_img.append(img)\n",
    "\n",
    "    for directory_path in train_ann_paths[:]:\n",
    "        img = cv2.imread(directory_path, 0)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = preprocessImg(img)\n",
    "        train_ann.append(img)\n",
    "\n",
    "    for directory_path in val_img_paths[:]:\n",
    "        img = cv2.imread(directory_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        val_img.append(img)\n",
    "\n",
    "    for directory_path in val_ann_paths[:]:\n",
    "        img = cv2.imread(directory_path, 0)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = preprocessImg(img)\n",
    "        val_ann.append(img)\n",
    "\n",
    "    train_img = np.array(train_img)\n",
    "    train_ann = np.array(train_ann)\n",
    "    val_img = np.array(val_img)\n",
    "    val_ann = np.array(val_ann)\n",
    "\n",
    "    return train_img, train_ann, val_img, val_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12f98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, train_ann, val_img, val_ann = prepare_images(\n",
    "    img_size=img_size,\n",
    "    train_img_paths=train_img_paths[:],\n",
    "    train_ann_paths=train_ann_paths[:],\n",
    "    val_img_paths=val_img_paths[:],\n",
    "    val_ann_paths=val_ann_paths[:]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f170ea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image: (2801, 160, 160, 3)\n",
      "train_ann (2801, 160, 160, 1)\n",
      "val_img (500, 160, 160, 3)\n",
      "val_ann (500, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train image:\", train_img.shape)\n",
    "print(\"train_ann\", train_ann.shape)\n",
    "print(\"val_img\", val_img.shape)\n",
    "print(\"val_ann\", val_ann.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c4db2",
   "metadata": {},
   "source": [
    "# Segmentation Models Library\n",
    "https://github.com/qubvel/segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2d66c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_unet(\n",
    "    backbone, callbacks, epochs, batch_size, train_img, train_ann,\n",
    "    val_img, val_ann, encoder_weights=True,\n",
    "    optimizer='rmsprop', loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[UpdatedMeanIoU(num_classes=8), SparseCategoricalAccuracy()], \n",
    "    activation='softmax'):\n",
    "    \n",
    "    BACKBONE = backbone\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "    X_train = preprocess_input(train_img)\n",
    "    X_test = preprocess_input(val_img)\n",
    "\n",
    "    if encoder_weights:\n",
    "        model = sm.Unet(BACKBONE, encoder_weights='imagenet', classes=num_classes)\n",
    "\n",
    "    else:\n",
    "        model = sm.Unet(BACKBONE, classes=num_classes)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(callbacks, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        train_ann,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_test, val_ann),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.now()\n",
    "\n",
    "history = segmentation_unet(\n",
    "    backbone='resnet34',\n",
    "    callbacks='Unet_resnet34_FL_160x160.h5',\n",
    "    epochs=15,\n",
    "    batch_size=16, \n",
    "    train_img=train_img,\n",
    "    train_ann=train_ann, \n",
    "    val_img=val_img,\n",
    "    val_ann=val_ann,\n",
    ")\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(running_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "723adcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aecef0",
   "metadata": {},
   "source": [
    "# Training model with image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3645685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_unet2(\n",
    "    backbone, callbacks, epochs, train_gen,\n",
    "    val_gen, encoder_weights=True,\n",
    "    optimizer='rmsprop', loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[UpdatedMeanIoU(num_classes=8), SparseCategoricalAccuracy()], \n",
    "    activation='softmax'):\n",
    "    \n",
    "    BACKBONE = backbone\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "    # X_train = preprocess_input(train_img)\n",
    "    # X_test = preprocess_input(val_img)\n",
    "\n",
    "    if encoder_weights:\n",
    "        model = sm.Unet(BACKBONE, encoder_weights='imagenet', classes=num_classes)\n",
    "\n",
    "    else:\n",
    "        model = sm.Unet(BACKBONE, classes=num_classes)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(callbacks, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # print(model.summary())\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8a25fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loulou\\AppData\\Local\\Temp\\ipykernel_14256\\1438588986.py:32: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "150/150 [==============================] - 261s 2s/step - loss: 0.7379 - updated_mean_io_u: 0.4492 - sparse_categorical_accuracy: 0.7744 - val_loss: 39.2329 - val_updated_mean_io_u: 0.3002 - val_sparse_categorical_accuracy: 0.5829\n",
      "Epoch 2/15\n",
      "150/150 [==============================] - 237s 2s/step - loss: 0.4836 - updated_mean_io_u: 0.5620 - sparse_categorical_accuracy: 0.8511 - val_loss: 14.1323 - val_updated_mean_io_u: 0.3248 - val_sparse_categorical_accuracy: 0.6290\n",
      "Epoch 3/15\n",
      "150/150 [==============================] - 241s 2s/step - loss: 0.4378 - updated_mean_io_u: 0.5992 - sparse_categorical_accuracy: 0.8650 - val_loss: 0.5516 - val_updated_mean_io_u: 0.5471 - val_sparse_categorical_accuracy: 0.8284\n",
      "Epoch 4/15\n",
      "150/150 [==============================] - 239s 2s/step - loss: 0.4065 - updated_mean_io_u: 0.6218 - sparse_categorical_accuracy: 0.8744 - val_loss: 0.5020 - val_updated_mean_io_u: 0.5796 - val_sparse_categorical_accuracy: 0.8430\n",
      "Epoch 5/15\n",
      "150/150 [==============================] - 236s 2s/step - loss: 0.3856 - updated_mean_io_u: 0.6365 - sparse_categorical_accuracy: 0.8804 - val_loss: 0.6770 - val_updated_mean_io_u: 0.5725 - val_sparse_categorical_accuracy: 0.8478\n",
      "Epoch 6/15\n",
      "150/150 [==============================] - 232s 2s/step - loss: 0.3677 - updated_mean_io_u: 0.6482 - sparse_categorical_accuracy: 0.8848 - val_loss: 0.5061 - val_updated_mean_io_u: 0.6021 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 7/15\n",
      "150/150 [==============================] - 236s 2s/step - loss: 0.3525 - updated_mean_io_u: 0.6581 - sparse_categorical_accuracy: 0.8891 - val_loss: 0.4701 - val_updated_mean_io_u: 0.6081 - val_sparse_categorical_accuracy: 0.8664\n",
      "Epoch 8/15\n",
      "150/150 [==============================] - 234s 2s/step - loss: 0.3381 - updated_mean_io_u: 0.6651 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.4412 - val_updated_mean_io_u: 0.6312 - val_sparse_categorical_accuracy: 0.8717\n",
      "Epoch 9/15\n",
      "150/150 [==============================] - 233s 2s/step - loss: 0.3243 - updated_mean_io_u: 0.6723 - sparse_categorical_accuracy: 0.8948 - val_loss: 0.4431 - val_updated_mean_io_u: 0.6294 - val_sparse_categorical_accuracy: 0.8681\n",
      "Epoch 10/15\n",
      "150/150 [==============================] - 231s 2s/step - loss: 0.3164 - updated_mean_io_u: 0.6779 - sparse_categorical_accuracy: 0.8968 - val_loss: 0.4703 - val_updated_mean_io_u: 0.6211 - val_sparse_categorical_accuracy: 0.8612\n",
      "Epoch 11/15\n",
      "150/150 [==============================] - 238s 2s/step - loss: 0.3053 - updated_mean_io_u: 0.6850 - sparse_categorical_accuracy: 0.9013 - val_loss: 0.4295 - val_updated_mean_io_u: 0.6251 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 12/15\n",
      "150/150 [==============================] - 244s 2s/step - loss: 0.2979 - updated_mean_io_u: 0.6893 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.5606 - val_updated_mean_io_u: 0.6073 - val_sparse_categorical_accuracy: 0.8607\n",
      "Epoch 13/15\n",
      "150/150 [==============================] - 243s 2s/step - loss: 0.2858 - updated_mean_io_u: 0.6965 - sparse_categorical_accuracy: 0.9073 - val_loss: 0.4893 - val_updated_mean_io_u: 0.6275 - val_sparse_categorical_accuracy: 0.8687\n",
      "Epoch 14/15\n",
      "150/150 [==============================] - 238s 2s/step - loss: 0.2791 - updated_mean_io_u: 0.7008 - sparse_categorical_accuracy: 0.9094 - val_loss: 0.4571 - val_updated_mean_io_u: 0.6318 - val_sparse_categorical_accuracy: 0.8668\n",
      "Epoch 15/15\n",
      "150/150 [==============================] - 236s 2s/step - loss: 0.2713 - updated_mean_io_u: 0.7056 - sparse_categorical_accuracy: 0.9117 - val_loss: 0.4849 - val_updated_mean_io_u: 0.6375 - val_sparse_categorical_accuracy: 0.8692\n",
      "3582\n"
     ]
    }
   ],
   "source": [
    "start = dt.now()\n",
    "\n",
    "history = segmentation_unet2(\n",
    "    train_gen=train_gen,\n",
    "    val_gen=val_gen,\n",
    "    backbone='resnet34',\n",
    "    callbacks='Unet_resnet34_FL_160x160.h5',\n",
    "    epochs=15, \n",
    "    \n",
    ")\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(running_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acacb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cee2a8bf78ed58fe7d4a6da4c01dc73c50fe79a52336feed85a35ce76368bd76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
